{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafío de Ingeniería de Datos — Análisis de Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparación del entorno\n",
    "\n",
    "En esta sección, aseguramos que tenemos los archivos necesarios, descargamos los datos desde drive si es necesario, convertimos en parquet para mejor aproveitamento con DuckDB y respondemos las preguntas.\n",
    "\n",
    "# Contexto y enfoque para resolver el desafío\n",
    "\n",
    "Este notebook documenta el proceso completo para resolver tres preguntas relacionadas con el análisis de un conjunto de tweets. A continuación se explica la lógica, las decisiones técnicas y las bibliotecas utilizadas para construir una solución eficiente y escalable.\n",
    "\n",
    "## Elección de herramientas y bibliotecas\n",
    "\n",
    "- **DuckDB:** Elegí DuckDB como motor de consultas SQL embebido en Python por varias razones:\n",
    "  - Permite ejecutar consultas SQL directamente sobre archivos Parquet y JSON sin necesidad de cargas complejas.\n",
    "  - Es muy eficiente para análisis de datos en columnas (columnar), lo que mejora el rendimiento al trabajar con grandes volúmenes de datos.\n",
    "  - Proporciona un lenguaje declarativo (SQL) para consultas complejas, facilitando la extracción y agregación de información.\n",
    "  - Comparado con leer archivos directamente con pandas, DuckDB es más rápido y consume menos memoria cuando se manejan grandes datasets.\n",
    "  - Evita la necesidad de construir procesos ETL o pipelines externos, manteniendo todo en un solo entorno de código.\n",
    "  - Mayor facilidad de toda la equipa en usar SQL para extraer informaciones.\n",
    "\n",
    "- **gdown:** Para descargar archivos desde Google Drive de forma automatizada y reproducible.\n",
    "\n",
    "- **emoji:** Para manusear emojis en contenido de cada tweet y extraer los más utilizados.\n",
    "\n",
    "## Razonamiento general del enfoque\n",
    "\n",
    "1. **Preparación de datos:** \n",
    "   - El dataset original está en formato JSON, con tweets almacenados línea a línea.\n",
    "   - Para consultas eficientes, convertimos este JSON a Parquet, un formato columnar optimizado para consultas analíticas, utilizando DuckDB por mejor performance y facilidad.\n",
    "\n",
    "2. **Consultas analíticas:**\n",
    "   - Utilizo SQL para filtrar, agrupar y ordenar los datos, logrando respuestas rápidas y claras a las preguntas.\n",
    "   - Uso expresiones regulares para extraer emojis y menciones dentro del texto de los tweets.\n",
    "   - Extraigo solo los datos relevantes para cada pregunta, mejorando el rendimiento.\n",
    "\n",
    "3. **Ventajas de esta solución:**\n",
    "   - Código limpio y mantenible.\n",
    "   - Uso de tecnologías que permiten escalabilidad a datasets más grandes.\n",
    "   - Facilidad para modificar y extender consultas futuras.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ejecución dos scripts\n",
    "\n",
    "1. Primero hacemos el download de lo arquivo json, salvo en un drive, usando a lib gdown. Esta funcion armazenará el arquivo json en la pasta data/. Caso el arquivo ya estea en la pasta, el script nos avisará e pulará el processo de download de lo arquivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de todo, tenemos que instalar las dependecias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_data import download_file_from_google_drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file_from_google_drive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Después, convertimos el arquivo json para formato parquet, devido a mejor performance para consultas columnares usando DuckDB. Otra véz, caso el arquivo parquet ya estea en la pasta data/, seremos avisados y lo processo de conversion no será ejecutado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convert_to_parquet import convert_json_to_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
