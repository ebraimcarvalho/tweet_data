{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DesafÃ­o de IngenierÃ­a de Datos â€” AnÃ¡lisis de Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PreparaciÃ³n del entorno\n",
    "\n",
    "En esta secciÃ³n, aseguramos que tenemos los archivos necesarios, descargamos los datos desde drive si es necesario, convertimos en parquet para mejor aproveitamento con DuckDB y respondemos las preguntas.\n",
    "\n",
    "# Contexto y enfoque para resolver el desafÃ­o\n",
    "\n",
    "Este notebook documenta el proceso completo para resolver tres preguntas relacionadas con el anÃ¡lisis de un conjunto de tweets. A continuaciÃ³n se explica la lÃ³gica, las decisiones tÃ©cnicas y las bibliotecas utilizadas para construir una soluciÃ³n eficiente y escalable.\n",
    "\n",
    "## ElecciÃ³n de herramientas y bibliotecas\n",
    "\n",
    "- **DuckDB:** ElegÃ­ DuckDB como motor de consultas SQL embebido en Python por varias razones:\n",
    "  - Permite ejecutar consultas SQL directamente sobre archivos Parquet y JSON sin necesidad de cargas complejas.\n",
    "  - Es muy eficiente para anÃ¡lisis de datos en columnas (columnar), lo que mejora el rendimiento al trabajar con grandes volÃºmenes de datos.\n",
    "  - Proporciona un lenguaje declarativo (SQL) para consultas complejas, facilitando la extracciÃ³n y agregaciÃ³n de informaciÃ³n.\n",
    "  - Comparado con leer archivos directamente con pandas, DuckDB es mÃ¡s rÃ¡pido y consume menos memoria cuando se manejan grandes datasets.\n",
    "  - Evita la necesidad de construir procesos ETL o pipelines externos, manteniendo todo en un solo entorno de cÃ³digo.\n",
    "  - Mayor facilidad de toda la equipa en usar SQL para extraer informaciones.\n",
    "\n",
    "- **gdown:** Para descargar archivos desde Google Drive de forma automatizada y reproducible.\n",
    "\n",
    "- **emoji:** Para manusear emojis en contenido de cada tweet y extraer los mÃ¡s utilizados.\n",
    "\n",
    "## Razonamiento general del enfoque\n",
    "\n",
    "1. **PreparaciÃ³n de datos:** \n",
    "   - El dataset original estÃ¡ en formato JSON, con tweets almacenados lÃ­nea a lÃ­nea.\n",
    "   - Para consultas eficientes, convertimos este JSON a Parquet, un formato columnar optimizado para consultas analÃ­ticas, utilizando DuckDB por mejor performance y facilidad.\n",
    "\n",
    "2. **Consultas analÃ­ticas:**\n",
    "   - Utilizo SQL para filtrar, agrupar y ordenar los datos, logrando respuestas rÃ¡pidas y claras a las preguntas.\n",
    "   - Uso expresiones regulares para extraer emojis y menciones dentro del texto de los tweets.\n",
    "   - Extraigo solo los datos relevantes para cada pregunta, mejorando el rendimiento.\n",
    "\n",
    "3. **Ventajas de esta soluciÃ³n:**\n",
    "   - CÃ³digo limpio y mantenible.\n",
    "   - Uso de tecnologÃ­as que permiten escalabilidad a datasets mÃ¡s grandes.\n",
    "   - Facilidad para modificar y extender consultas futuras.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EjecuciÃ³n dos scripts\n",
    "\n",
    "1. Primero hacemos el download de lo arquivo json, salvo en un drive, usando a lib gdown. Esta funcion armazenarÃ¡ el arquivo json en la pasta data/. Caso el arquivo ya estea en la pasta, el script nos avisarÃ¡ e pularÃ¡ el processo de download de lo arquivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de todo, tenemos que instalar las dependecias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ebraim/Documentos/projetos/tweet_data/src'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting memory-profiler==0.61.0 (from -r ../requirements.txt (line 1))\n",
      "  Using cached memory_profiler-0.61.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: beautifulsoup4==4.13.4 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 2)) (4.13.4)\n",
      "Requirement already satisfied: certifi==2025.4.26 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 3)) (2025.4.26)\n",
      "Requirement already satisfied: charset-normalizer==3.4.2 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 4)) (3.4.2)\n",
      "Requirement already satisfied: duckdb==1.2.2 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 5)) (1.2.2)\n",
      "Requirement already satisfied: emoji==2.14.1 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 6)) (2.14.1)\n",
      "Requirement already satisfied: filelock==3.16.1 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 7)) (3.16.1)\n",
      "Requirement already satisfied: gdown==5.2.0 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 8)) (5.2.0)\n",
      "Requirement already satisfied: idna==3.10 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 9)) (3.10)\n",
      "Requirement already satisfied: numpy==1.24.4 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 10)) (1.24.4)\n",
      "Requirement already satisfied: pyarrow==17.0.0 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 11)) (17.0.0)\n",
      "Requirement already satisfied: PySocks==1.7.1 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 12)) (1.7.1)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 13)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz==2025.2 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 14)) (2025.2)\n",
      "Requirement already satisfied: requests==2.32.3 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 15)) (2.32.3)\n",
      "Requirement already satisfied: six==1.17.0 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 16)) (1.17.0)\n",
      "Requirement already satisfied: soupsieve==2.7 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 17)) (2.7)\n",
      "Requirement already satisfied: tqdm==4.67.1 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 18)) (4.67.1)\n",
      "Requirement already satisfied: typing_extensions==4.13.2 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 19)) (4.13.2)\n",
      "Requirement already satisfied: tzdata==2025.2 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 20)) (2025.2)\n",
      "Requirement already satisfied: urllib3==2.2.3 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 21)) (2.2.3)\n",
      "Requirement already satisfied: psutil in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from memory-profiler==0.61.0->-r ../requirements.txt (line 1)) (7.0.0)\n",
      "Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: memory-profiler\n",
      "Successfully installed memory-profiler-0.61.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_data import download_file_from_google_drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "challenge.ipynb        __init__.py   q1_time.py    q3_memory.py\n",
      "convert_to_parquet.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/  q2_memory.py  q3_time.py\n",
      "download_data.py       q1_memory.py  q2_time.py\n"
     ]
    }
   ],
   "source": [
    "%ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… File already exists. Skipping download.\n",
      "Elapsed: 0.0034 seconds\n"
     ]
    }
   ],
   "source": [
    "download_file_from_google_drive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. DespuÃ©s, convertimos el arquivo json para formato parquet, devido a mejor performance para consultas columnares usando DuckDB. Otra vÃ©z, caso el arquivo parquet ya estea en la pasta data/, seremos avisados y lo processo de conversion no serÃ¡ ejecutado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convert_to_parquet import convert_json_to_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Converting JSON to Parquet...\n"
     ]
    },
    {
     "ename": "IOException",
     "evalue": "IO Error: No files found that match the pattern \"data/farmers-protest-tweets-2021-2-4.json\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOException\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconvert_json_to_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/projetos/tweet_data/src/convert_to_parquet.py:15\u001b[0m, in \u001b[0;36mconvert_json_to_parquet\u001b[0;34m(json_path, parquet_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROJECT_ROOT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparquet_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ”„ Converting JSON to Parquet...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mduckdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;43m        COPY (\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;43m            SELECT * FROM read_json_auto(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mjson_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;43m        ) TO \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mparquet_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m (FORMAT PARQUET)\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Parquet file created:\u001b[39m\u001b[38;5;124m\"\u001b[39m, parquet_path)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mIOException\u001b[0m: IO Error: No files found that match the pattern \"data/farmers-protest-tweets-2021-2-4.json\""
     ]
    }
   ],
   "source": [
    "convert_json_to_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv tweet data)",
   "language": "python",
   "name": "venv-tweet-data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
