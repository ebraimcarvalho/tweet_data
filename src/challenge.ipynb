{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafío de Ingeniería de Datos — Análisis de Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparación del entorno\n",
    "\n",
    "En esta sección, aseguramos que tenemos los archivos necesarios, descargamos los datos desde drive si es necesario, convertimos en parquet para mejor aproveitamento con DuckDB y respondemos las preguntas.\n",
    "\n",
    "# Contexto y enfoque para resolver el desafío\n",
    "\n",
    "Este notebook documenta el proceso completo para resolver tres preguntas relacionadas con el análisis de un conjunto de tweets. A continuación se explica la lógica, las decisiones técnicas y las bibliotecas utilizadas para construir una solución eficiente y escalable.\n",
    "\n",
    "## Elección de herramientas y bibliotecas\n",
    "\n",
    "- **DuckDB:** Elegí DuckDB como motor de consultas SQL embebido en Python por varias razones:\n",
    "  - Permite ejecutar consultas SQL directamente sobre archivos Parquet y JSON sin necesidad de cargas complejas.\n",
    "  - Es muy eficiente para análisis de datos en columnas (columnar), lo que mejora el rendimiento al trabajar con grandes volúmenes de datos.\n",
    "  - Proporciona un lenguaje declarativo (SQL) para consultas complejas, facilitando la extracción y agregación de información.\n",
    "  - Comparado con leer archivos directamente con pandas, DuckDB es más rápido y consume menos memoria cuando se manejan grandes datasets.\n",
    "  - Evita la necesidad de construir procesos ETL o pipelines externos, manteniendo todo en un solo entorno de código.\n",
    "  - Mayor facilidad de toda la equipa en usar SQL para extraer informaciones.\n",
    "\n",
    "- **gdown:** Para descargar archivos desde Google Drive de forma automatizada y reproducible.\n",
    "\n",
    "- **emoji:** Para manusear emojis en contenido de cada tweet y extraer los más utilizados.\n",
    "\n",
    "## Razonamiento general del enfoque\n",
    "\n",
    "1. **Preparación de datos:** \n",
    "   - El dataset original está en formato JSON, con tweets almacenados línea a línea.\n",
    "   - Para consultas eficientes, convertimos este JSON a Parquet, un formato columnar optimizado para consultas analíticas, utilizando DuckDB por mejor performance y facilidad.\n",
    "\n",
    "2. **Consultas analíticas:**\n",
    "   - Utilizo SQL para filtrar, agrupar y ordenar los datos, logrando respuestas rápidas y claras a las preguntas.\n",
    "   - Uso expresiones regulares para extraer emojis y menciones dentro del texto de los tweets.\n",
    "   - Extraigo solo los datos relevantes para cada pregunta, mejorando el rendimiento.\n",
    "\n",
    "3. **Ventajas de esta solución:**\n",
    "   - Código limpio y mantenible.\n",
    "   - Uso de tecnologías que permiten escalabilidad a datasets más grandes.\n",
    "   - Facilidad para modificar y extender consultas futuras.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ejecución dos scripts\n",
    "\n",
    "1. Primero hacemos el download de lo arquivo json, salvo en un drive, usando a lib gdown. Esta funcion armazenará el arquivo json en la pasta data/. Caso el arquivo ya estea en la pasta, el script nos avisará e pulará el processo de download de lo arquivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de todo, tenemos que instalar las dependecias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ebraim/Documentos/projetos/tweet_data/src'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting memory-profiler==0.61.0 (from -r ../requirements.txt (line 1))\n",
      "  Using cached memory_profiler-0.61.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: beautifulsoup4==4.13.4 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 2)) (4.13.4)\n",
      "Requirement already satisfied: certifi==2025.4.26 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 3)) (2025.4.26)\n",
      "Requirement already satisfied: charset-normalizer==3.4.2 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 4)) (3.4.2)\n",
      "Requirement already satisfied: duckdb==1.2.2 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 5)) (1.2.2)\n",
      "Requirement already satisfied: emoji==2.14.1 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 6)) (2.14.1)\n",
      "Requirement already satisfied: filelock==3.16.1 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 7)) (3.16.1)\n",
      "Requirement already satisfied: gdown==5.2.0 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 8)) (5.2.0)\n",
      "Requirement already satisfied: idna==3.10 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 9)) (3.10)\n",
      "Requirement already satisfied: numpy==1.24.4 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 10)) (1.24.4)\n",
      "Requirement already satisfied: pyarrow==17.0.0 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 11)) (17.0.0)\n",
      "Requirement already satisfied: PySocks==1.7.1 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 12)) (1.7.1)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 13)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz==2025.2 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 14)) (2025.2)\n",
      "Requirement already satisfied: requests==2.32.3 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 15)) (2.32.3)\n",
      "Requirement already satisfied: six==1.17.0 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 16)) (1.17.0)\n",
      "Requirement already satisfied: soupsieve==2.7 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 17)) (2.7)\n",
      "Requirement already satisfied: tqdm==4.67.1 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 18)) (4.67.1)\n",
      "Requirement already satisfied: typing_extensions==4.13.2 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 19)) (4.13.2)\n",
      "Requirement already satisfied: tzdata==2025.2 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 20)) (2025.2)\n",
      "Requirement already satisfied: urllib3==2.2.3 in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 21)) (2.2.3)\n",
      "Requirement already satisfied: psutil in /home/ebraim/Documentos/projetos/tweet_data/venv/lib/python3.8/site-packages (from memory-profiler==0.61.0->-r ../requirements.txt (line 1)) (7.0.0)\n",
      "Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: memory-profiler\n",
      "Successfully installed memory-profiler-0.61.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_data import download_file_from_google_drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "challenge.ipynb        __init__.py   q1_time.py    q3_memory.py\n",
      "convert_to_parquet.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/  q2_memory.py  q3_time.py\n",
      "download_data.py       q1_memory.py  q2_time.py\n"
     ]
    }
   ],
   "source": [
    "%ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️ Downloading file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1B-TywPDU-BBFbMFbrqy2v71FrFxUqsa7\n",
      "From (redirected): https://drive.google.com/uc?id=1B-TywPDU-BBFbMFbrqy2v71FrFxUqsa7&confirm=t&uuid=e72278b4-1e35-4d9f-bae5-3eca547729e4\n",
      "To: /home/ebraim/Documentos/projetos/tweet_data/data/farmers-protest-tweets-2021-2-4.json\n",
      "100%|██████████| 408M/408M [01:54<00:00, 3.56MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 119.2029 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "download_file_from_google_drive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Después, convertimos el arquivo json para formato parquet, devido a mejor performance para consultas columnares usando DuckDB. Otra véz, caso el arquivo parquet ya estea en la pasta data/, seremos avisados y lo processo de conversion no será ejecutado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convert_to_parquet import convert_json_to_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Converting JSON to Parquet...\n",
      "✅ Parquet file created: /home/ebraim/Documentos/projetos/tweet_data/data/farmers.parquet\n",
      "Elapsed: 16.7176 seconds\n"
     ]
    }
   ],
   "source": [
    "convert_json_to_parquet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Agora tenemos el arquivo .parquet para hacer consultas optimizadas con duckdb, entoncens podemos responder las preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Las top 10 fechas donde hay más tweets. Mencionar el usuario (username) que más publicaciones tiene por cada uno de esos días. Debe incluir las siguientes funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import q1_time as q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 19), 'Preetm91'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 20), 'MangalJ23056160')]\n",
      "Elapsed: 0.5506 seconds\n"
     ]
    }
   ],
   "source": [
    "q1.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Los top 10 emojis más usados con su respectivo conteo. Debe incluir las siguientes funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import q2_time as q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('🙏', 7286), ('😂', 3072), ('🚜', 2972), ('✊', 2411), ('🌾', 2363), ('🏻', 2080), ('❤', 1779), ('🤣', 1668), ('🏽', 1218), ('👇', 1108)]\n",
      "Elapsed: 7.2251 seconds\n"
     ]
    }
   ],
   "source": [
    "q2.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. El top 10 histórico de usuarios (username) más influyentes en función del conteo de las menciones (@) que registra cada uno de ellos. Debe incluir las siguientes funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import q3_time as q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2261), ('Kisanektamorcha', 1836), ('RakeshTikaitBKU', 1641), ('PMOIndia', 1422), ('RahulGandhi', 1125), ('GretaThunberg', 1046), ('RaviSinghKA', 1015), ('rihanna', 972), ('UNHumanRights', 962), ('meenaharris', 925)]\n",
      "Elapsed: 1.6351 seconds\n"
     ]
    }
   ],
   "source": [
    "q3.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import q1_memory as q1_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/ebraim/Documentos/projetos/tweet_data/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     7     71.3 MiB     71.3 MiB           1   @profile\n",
      "     8                                         def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "     9                                         \n",
      "    10     71.3 MiB      0.0 MiB           1       CURRENT_FOLDER = Path(__file__).resolve()\n",
      "    11     71.3 MiB      0.0 MiB           1       PROJECT_ROOT = CURRENT_FOLDER.parent.parent\n",
      "    12                                         \n",
      "    13     84.7 MiB     13.4 MiB           1       con = duckdb.connect(database=':memory:')\n",
      "    14                                         \n",
      "    15     99.5 MiB     14.8 MiB           3       con.execute(f\"\"\"\n",
      "    16                                                 CREATE TABLE tweets AS \n",
      "    17                                                 SELECT \n",
      "    18                                                     CAST(SUBSTR(date, 1, 10) AS DATE) AS tweet_date,\n",
      "    19                                                     user['username'] AS username\n",
      "    20     84.7 MiB      0.0 MiB           2           FROM read_parquet('{PROJECT_ROOT}/{file_path}')\n",
      "    21                                             \"\"\")\n",
      "    22                                         \n",
      "    23     99.5 MiB      0.0 MiB           1       query = \"\"\"\n",
      "    24                                             WITH tweet_counts AS (\n",
      "    25                                                 SELECT tweet_date, COUNT(1) AS cnt\n",
      "    26                                                 FROM tweets\n",
      "    27                                                 GROUP BY tweet_date\n",
      "    28                                                 ORDER BY cnt DESC\n",
      "    29                                                 LIMIT 10\n",
      "    30                                             ),\n",
      "    31                                             user_counts AS (\n",
      "    32                                                 SELECT tweet_date, username, count(1) AS tweet_count\n",
      "    33                                                 FROM tweets\n",
      "    34                                                 WHERE tweet_date IN (SELECT tweet_date FROM tweet_counts)\n",
      "    35                                                 GROUP BY tweet_date, username\n",
      "    36                                             ),\n",
      "    37                                             ranked_users AS (\n",
      "    38                                                 SELECT *,\n",
      "    39                                                     ROW_NUMBER() OVER (PARTITION BY tweet_date ORDER BY tweet_count DESC) AS rn\n",
      "    40                                                 FROM user_counts\n",
      "    41                                                 QUALIFY rn = 1\n",
      "    42                                             )\n",
      "    43                                             SELECT tweet_date, username\n",
      "    44                                             FROM ranked_users\n",
      "    45                                             ORDER BY tweet_count DESC\n",
      "    46                                             \"\"\"\n",
      "    47                                         \n",
      "    48    110.8 MiB     11.4 MiB           1       result = con.execute(query).fetchall()\n",
      "    49     91.5 MiB    -19.3 MiB           1       con.close()\n",
      "    50     91.5 MiB      0.0 MiB           1       return result\n",
      "\n",
      "\n",
      "[(datetime.date(2021, 2, 19), 'Preetm91'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 20), 'MangalJ23056160')]\n"
     ]
    }
   ],
   "source": [
    "q1_m.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import q2_memory as q2_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/ebraim/Documentos/projetos/tweet_data/src/q2_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    11     75.0 MiB     75.0 MiB           1   @profile\n",
      "    12                                         def q2_time(file_path):\n",
      "    13                                         \n",
      "    14     75.0 MiB      0.0 MiB           1       CURRENT_FOLDER = Path(__file__).resolve()\n",
      "    15     75.0 MiB      0.0 MiB           1       PROJECT_ROOT = CURRENT_FOLDER.parent.parent\n",
      "    16                                         \n",
      "    17    224.7 MiB    149.8 MiB           1       df = duckdb.query(f\"SELECT content FROM read_parquet('{PROJECT_ROOT}/{file_path}')\").to_df()\n",
      "    18                                         \n",
      "    19    224.7 MiB      0.0 MiB           1       all_emojis = []\n",
      "    20                                         \n",
      "    21    231.5 MiB      4.5 MiB      117408       for content in df['content'].dropna():\n",
      "    22    231.5 MiB      2.3 MiB      117407           all_emojis.extend(extract_emojis(content))\n",
      "    23                                         \n",
      "    24    231.5 MiB      0.0 MiB           1       counter = Counter(all_emojis)\n",
      "    25    231.5 MiB      0.0 MiB           1       return counter.most_common(10)\n",
      "\n",
      "\n",
      "[('🙏', 7286), ('😂', 3072), ('🚜', 2972), ('✊', 2411), ('🌾', 2363), ('🏻', 2080), ('❤', 1779), ('🤣', 1668), ('🏽', 1218), ('👇', 1108)]\n"
     ]
    }
   ],
   "source": [
    "q2_m.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import q3_memory as q3_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/ebraim/Documentos/projetos/tweet_data/src/q3_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     7     89.8 MiB     89.8 MiB           1   @profile\n",
      "     8                                         def q3_time(file_path):\n",
      "     9                                         \n",
      "    10     89.8 MiB      0.0 MiB           1       CURRENT_FOLDER = Path(__file__).resolve()\n",
      "    11     89.8 MiB      0.0 MiB           1       PROJECT_ROOT = CURRENT_FOLDER.parent.parent\n",
      "    12                                             \n",
      "    13     89.8 MiB      0.0 MiB           3       query = f\"\"\"\n",
      "    14                                                 WITH mentions_extracted AS (\n",
      "    15                                                     SELECT REGEXP_EXTRACT_ALL(content, '@\\\\w+') AS mentions\n",
      "    16     89.8 MiB      0.0 MiB           2               FROM read_parquet('{PROJECT_ROOT}/{file_path}')\n",
      "    17                                                 ),\n",
      "    18                                                 mentions_flat AS (\n",
      "    19                                                     SELECT unnest(mentions) AS mention\n",
      "    20                                                     FROM mentions_extracted\n",
      "    21                                                 ),\n",
      "    22                                                 mentions_counted AS (\n",
      "    23                                                     SELECT mention, COUNT(*) AS count\n",
      "    24                                                     FROM mentions_flat\n",
      "    25                                                     GROUP BY mention\n",
      "    26                                                     ORDER BY count DESC\n",
      "    27                                                     LIMIT 10\n",
      "    28                                                 )\n",
      "    29                                                 SELECT REPLACE(mention, '@', '') as username_mentioned, count FROM mentions_counted\n",
      "    30                                             \"\"\"\n",
      "    31    127.3 MiB     37.5 MiB           1       return duckdb.execute(query).fetchall()\n",
      "\n",
      "\n",
      "[('narendramodi', 2261), ('Kisanektamorcha', 1836), ('RakeshTikaitBKU', 1641), ('PMOIndia', 1422), ('RahulGandhi', 1125), ('GretaThunberg', 1046), ('RaviSinghKA', 1015), ('rihanna', 972), ('UNHumanRights', 962), ('meenaharris', 925)]\n"
     ]
    }
   ],
   "source": [
    "q3_m.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv tweet data)",
   "language": "python",
   "name": "venv-tweet-data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
