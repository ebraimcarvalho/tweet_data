{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desaf√≠o de Ingenier√≠a de Datos ‚Äî An√°lisis de Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparaci√≥n del entorno y contexto\n",
    "\n",
    "En esta secci√≥n, nos aseguramos de tener los archivos necesarios, descargamos los datos de Drive si es necesario, los convertimos a parquet para un mejor uso con DuckDB y respondemos preguntas.\n",
    "\n",
    "### Contexto y enfoque para resolver el desaf√≠o\n",
    "\n",
    "Este notebook documenta todo el proceso de soluci√≥n del desaf√≠o de an√°lisis de tweets. Abarca desde la obtenci√≥n de datos hasta la optimizaci√≥n del procesamiento y la evaluaci√≥n del rendimiento en tiempo y memoria.\n",
    "\n",
    "Recibimos un archivo JSON con tweets relacionados con las protestas agr√≠colas en India. El objetivo era responder a tres preguntas espec√≠ficas relacionadas con fechas, emojis y usuarios mencionados. Busqu√© una soluci√≥n que fuera:\n",
    "\n",
    "- Eficiente en tiempo y memoria.\n",
    "\n",
    "- F√°cil de mantener y reproducir.\n",
    "\n",
    "- Basada en herramientas modernas y adecuadas para an√°lisis de datos.\n",
    "\n",
    "### Decisiones t√©cnicas y bibliotecas utilizadas\n",
    "\n",
    "- **DuckDB:** Consultas anal√≠ticas SQL directamente sobre archivos Parquet y JSON. Eleg√≠ DuckDB como motor de consultas SQL embebido en Python por varias razones:\n",
    "  - Permite ejecutar consultas SQL directamente sobre archivos Parquet y JSON sin necesidad de cargas complejas.\n",
    "  - Es muy eficiente para an√°lisis de datos en columnas (columnar), lo que mejora el rendimiento al trabajar con grandes vol√∫menes de datos.\n",
    "  - Proporciona un lenguaje declarativo (SQL) para consultas complejas, facilitando la extracci√≥n y agregaci√≥n de informaci√≥n.\n",
    "  - Comparado con leer archivos directamente con pandas, DuckDB es m√°s r√°pido y consume menos memoria cuando se manejan grandes datasets.\n",
    "  - Evita la necesidad de construir procesos ETL o pipelines externos, manteniendo todo en un solo entorno de c√≥digo.\n",
    "  - Mayor facilidad de toda la equipa en usar SQL para extraer informaciones.\n",
    "\n",
    "- **gdown:** Para descargar archivos desde Google Drive de forma automatizada y reproducible.\n",
    "\n",
    "- **emoji:** Para manusear emojis en contenido de cada tweet y extraer los m√°s utilizados.\n",
    "\n",
    "- **memory_profiler:** Medici√≥n precisa del uso de memoria por funci√≥n.\n",
    "\n",
    "- **time / cProfile:** Medici√≥n de tiempo de ejecuci√≥n.\n",
    "\n",
    "### Razonamiento general del enfoque\n",
    "\n",
    "1. **Preparaci√≥n de datos:** \n",
    "   - El dataset original est√° en formato JSON, con tweets almacenados l√≠nea a l√≠nea.\n",
    "   - Para consultas eficientes, convertimos este JSON a Parquet, un formato columnar optimizado para consultas anal√≠ticas, utilizando DuckDB por mejor performance y facilidad.\n",
    "\n",
    "2. **Consultas anal√≠ticas:**\n",
    "   - Utilizo SQL para filtrar, agrupar y ordenar los datos, logrando respuestas r√°pidas y claras a las preguntas.\n",
    "   - Uso expresiones regulares para extraer emojis y menciones dentro del texto de los tweets.\n",
    "   - Extraigo solo los datos relevantes para cada pregunta, mejorando el rendimiento.\n",
    "\n",
    "3. **Ventajas de esta soluci√≥n:**\n",
    "   - C√≥digo limpio y mantenible.\n",
    "   - Uso de tecnolog√≠as que permiten escalabilidad a datasets m√°s grandes.\n",
    "   - Facilidad para modificar y extender consultas futuras.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ejecuci√≥n dos scripts de preparaci√≥n\n",
    "\n",
    "Descarga del archivo JSON: Primero hacemos el download de lo arquivo json, salvo en un drive, usando a lib gdown. Esta funcion armazenar√° el arquivo json en la pasta data/. Caso el arquivo ya estea en la pasta, el script nos avisar√° e pular√° el processo de download de lo arquivo.\n",
    "\n",
    "Antes de todo, tenemos que instalar las dependecias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_data import download_file_from_google_drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File already exists. Skipping download.\n",
      "Elapsed: 0.0032 seconds\n"
     ]
    }
   ],
   "source": [
    "download_file_from_google_drive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversi√≥n de JSON a Parquet: Esta funci√≥n usa DuckDB para leer el archivo JSON l√≠nea por l√≠nea (read_json_auto) y escribirlo como Parquet (COPY TO ... (FORMAT PARQUET)), lo cual mejora significativamente el rendimiento de las consultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convert_to_parquet import convert_json_to_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Parquet file already exists.\n",
      "Elapsed: 0.0016 seconds\n"
     ]
    }
   ],
   "source": [
    "convert_json_to_parquet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Respuestas a las preguntas (tiempo de ejecuci√≥n)\n",
    "\n",
    "Agora tenemos el arquivo .parquet para hacer consultas optimizadas con duckdb, entoncens podemos responder las preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1. Las top 10 fechas donde hay m√°s tweets. Mencionar el usuario (username) que m√°s publicaciones tiene por cada uno de esos d√≠as. Debe incluir las siguientes funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import q1_time as q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 19), 'Preetm91'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 20), 'MangalJ23056160')]\n",
      "Elapsed: 0.1369 seconds\n"
     ]
    }
   ],
   "source": [
    "q1.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usamos DuckDB para extraer la fecha (date) a partir del campo date del tweet, agrupamos por fecha contando la cantidad de tweets para cada fecha.\n",
    "\n",
    "- Agrupamos por fecha y por username, contando la cantidad de tweets.\n",
    "\n",
    "- Luego, para cada fecha, usamos una subconsulta para seleccionar el usuario con m√°s publicaciones.\n",
    "\n",
    "- Se devuelve el top 10 de fechas con m√°s tweets y su usuario m√°s activo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2. Los top 10 emojis m√°s usados con su respectivo conteo. Debe incluir las siguientes funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import q2_time as q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('üôè', 7286), ('üòÇ', 3072), ('üöú', 2972), ('‚úä', 2411), ('üåæ', 2363), ('üèª', 2080), ('‚ù§', 1779), ('ü§£', 1668), ('üèΩ', 1218), ('üëá', 1108)]\n",
      "Elapsed: 1.4969 seconds\n"
     ]
    }
   ],
   "source": [
    "q2.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extraemos solo la columna content usando DuckDB.\n",
    "\n",
    "- Iteramos sobre cada tweet no nulo y usamos la biblioteca emoji para detectar y extraer todos los emojis.\n",
    "\n",
    "- Guardamos todos los emojis en una lista y usamos collections.Counter para contar las ocurrencias.\n",
    "\n",
    "- Devolvemos los 10 emojis m√°s frecuentes con su respectivo conteo.\n",
    "\n",
    "‚ö†Ô∏è Esta funci√≥n fue optimizada para ignorar caracteres comunes y s√≥lo contar emojis v√°lidos seg√∫n el est√°ndar Unicode Emoji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3. El top 10 hist√≥rico de usuarios (username) m√°s influyentes en funci√≥n del conteo de las menciones (@) que registra cada uno de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import q3_time as q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2261), ('Kisanektamorcha', 1836), ('RakeshTikaitBKU', 1641), ('PMOIndia', 1422), ('RahulGandhi', 1125), ('GretaThunberg', 1046), ('RaviSinghKA', 1015), ('rihanna', 972), ('UNHumanRights', 962), ('meenaharris', 925)]\n",
      "Elapsed: 0.1868 seconds\n"
     ]
    }
   ],
   "source": [
    "q3.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extraemos la columna content con DuckDB.\n",
    "\n",
    "- Usamos expresiones regulares para detectar menciones del tipo @usuario.\n",
    "\n",
    "- Para cada tweet, recolectamos las menciones y las agregamos.\n",
    "\n",
    "- Contamos las ocurrencias de cada usuario mencionado y devolvemos el top 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Medici√≥n de memoria utilizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import q1_memory as q1_m\n",
    "import q2_memory as q2_m\n",
    "import q3_memory as q3_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: c:\\Users\\ebraim\\Documents\\personal\\challenge_DE\\src\\q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     7     76.6 MiB     76.6 MiB           1   @profile\n",
      "     8                                         def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "     9                                         \n",
      "    10     76.6 MiB      0.0 MiB           1       CURRENT_FOLDER = Path(__file__).resolve()\n",
      "    11     76.6 MiB      0.0 MiB           1       PROJECT_ROOT = CURRENT_FOLDER.parent.parent\n",
      "    12                                         \n",
      "    13     85.0 MiB      8.4 MiB           1       con = duckdb.connect(database=':memory:')\n",
      "    14                                         \n",
      "    15     90.7 MiB      5.7 MiB           2       con.execute(f\"\"\"\n",
      "    16                                                 CREATE TABLE tweets AS \n",
      "    17                                                 SELECT \n",
      "    18                                                     CAST(SUBSTR(date, 1, 10) AS DATE) AS tweet_date,\n",
      "    19                                                     user['username'] AS username\n",
      "    20     85.0 MiB      0.0 MiB           1           FROM read_parquet('{PROJECT_ROOT}/{file_path}')\n",
      "    21                                             \"\"\")\n",
      "    22                                         \n",
      "    23     90.7 MiB      0.0 MiB           1       query = \"\"\"\n",
      "    24                                             WITH tweet_counts AS (\n",
      "    25                                                 SELECT tweet_date, COUNT(1) AS cnt\n",
      "    26                                                 FROM tweets\n",
      "    27                                                 GROUP BY tweet_date\n",
      "    28                                                 ORDER BY cnt DESC\n",
      "    29                                                 LIMIT 10\n",
      "    30                                             ),\n",
      "    31                                             user_counts AS (\n",
      "    32                                                 SELECT tweet_date, username, count(1) AS tweet_count\n",
      "    33                                                 FROM tweets\n",
      "    34                                                 WHERE tweet_date IN (SELECT tweet_date FROM tweet_counts)\n",
      "    35                                                 GROUP BY tweet_date, username\n",
      "    36                                             ),\n",
      "    37                                             ranked_users AS (\n",
      "    38                                                 SELECT *,\n",
      "    39                                                     ROW_NUMBER() OVER (PARTITION BY tweet_date ORDER BY tweet_count DESC) AS rn\n",
      "    40                                                 FROM user_counts\n",
      "    41                                                 QUALIFY rn = 1\n",
      "    42                                             )\n",
      "    43                                             SELECT tweet_date, username\n",
      "    44                                             FROM ranked_users\n",
      "    45                                             ORDER BY tweet_count DESC\n",
      "    46                                             \"\"\"\n",
      "    47                                         \n",
      "    48     94.3 MiB      3.6 MiB           1       result = con.execute(query).fetchall()\n",
      "    49     88.5 MiB     -5.8 MiB           1       con.close()\n",
      "    50     88.5 MiB      0.0 MiB           1       return result\n",
      "\n",
      "\n",
      "[(datetime.date(2021, 2, 19), 'Preetm91'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 20), 'MangalJ23056160')]\n",
      "peak memory: 91.62 MiB, increment: 16.51 MiB\n"
     ]
    }
   ],
   "source": [
    "%load_ext memory_profiler\n",
    "%memit q1_m.execute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n",
      "Filename: c:\\Users\\ebraim\\Documents\\personal\\challenge_DE\\src\\q2_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    11     88.5 MiB     88.5 MiB           1   @profile\n",
      "    12                                         def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    13                                         \n",
      "    14     88.5 MiB      0.0 MiB           1       CURRENT_FOLDER = Path(__file__).resolve()\n",
      "    15     88.5 MiB      0.0 MiB           1       PROJECT_ROOT = CURRENT_FOLDER.parent.parent\n",
      "    16                                         \n",
      "    17    180.1 MiB     91.5 MiB           1       df = duckdb.query(f\"SELECT content FROM read_parquet('{PROJECT_ROOT}/{file_path}')\").to_df()\n",
      "    18                                         \n",
      "    19    180.1 MiB      0.0 MiB           1       all_emojis = []\n",
      "    20                                         \n",
      "    21    185.0 MiB   -182.2 MiB      117408       for content in df['content'].dropna():\n",
      "    22    185.0 MiB   -181.4 MiB      117407           all_emojis.extend(extract_emojis(content))\n",
      "    23                                         \n",
      "    24    185.0 MiB      0.0 MiB           1       counter = Counter(all_emojis)\n",
      "    25    185.0 MiB      0.0 MiB           1       return counter.most_common(10)\n",
      "\n",
      "\n",
      "[('üôè', 7286), ('üòÇ', 3072), ('üöú', 2972), ('‚úä', 2411), ('üåæ', 2363), ('üèª', 2080), ('‚ù§', 1779), ('ü§£', 1668), ('üèΩ', 1218), ('üëá', 1108)]\n",
      "peak memory: 184.97 MiB, increment: 96.48 MiB\n"
     ]
    }
   ],
   "source": [
    "%load_ext memory_profiler\n",
    "%memit q2_m.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n",
      "Filename: c:\\Users\\ebraim\\Documents\\personal\\challenge_DE\\src\\q3_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     6    171.7 MiB    171.7 MiB           1   @profile\n",
      "     7                                         def q3_time(file_path):\n",
      "     8                                         \n",
      "     9    171.7 MiB      0.0 MiB           1       CURRENT_FOLDER = Path(__file__).resolve()\n",
      "    10    171.7 MiB      0.0 MiB           1       PROJECT_ROOT = CURRENT_FOLDER.parent.parent\n",
      "    11                                             \n",
      "    12    171.7 MiB      0.0 MiB           2       query = f\"\"\"\n",
      "    13                                                 WITH mentions_extracted AS (\n",
      "    14                                                     SELECT REGEXP_EXTRACT_ALL(content, '@\\\\w+') AS mentions\n",
      "    15    171.7 MiB      0.0 MiB           1               FROM read_parquet('{PROJECT_ROOT}/{file_path}')\n",
      "    16                                                 ),\n",
      "    17                                                 mentions_flat AS (\n",
      "    18                                                     SELECT unnest(mentions) AS mention\n",
      "    19                                                     FROM mentions_extracted\n",
      "    20                                                 ),\n",
      "    21                                                 mentions_counted AS (\n",
      "    22                                                     SELECT mention, COUNT(*) AS count\n",
      "    23                                                     FROM mentions_flat\n",
      "    24                                                     GROUP BY mention\n",
      "    25                                                     ORDER BY count DESC\n",
      "    26                                                     LIMIT 10\n",
      "    27                                                 )\n",
      "    28                                                 SELECT REPLACE(mention, '@', '') as username_mentioned, count FROM mentions_counted\n",
      "    29                                             \"\"\"\n",
      "    30    172.0 MiB      0.3 MiB           1       return duckdb.execute(query).fetchall()\n",
      "\n",
      "\n",
      "[('narendramodi', 2261), ('Kisanektamorcha', 1836), ('RakeshTikaitBKU', 1641), ('PMOIndia', 1422), ('RahulGandhi', 1125), ('GretaThunberg', 1046), ('RaviSinghKA', 1015), ('rihanna', 972), ('UNHumanRights', 962), ('meenaharris', 925)]\n",
      "peak memory: 175.46 MiB, increment: 3.76 MiB\n"
     ]
    }
   ],
   "source": [
    "%load_ext memory_profiler\n",
    "%memit q3_m.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DuckDB demostr√≥ ser una herramienta poderosa para an√°lisis columnar r√°pido y eficiente.\n",
    "\n",
    "- El uso de archivos Parquet permiti√≥ gran ahorro de memoria y una performance surpreendente para tiempo de ejecuci√≥n.\n",
    "\n",
    "- Las expresiones regulares y bibliotecas especializadas como emoji facilitaron el procesamiento textual.\n",
    "\n",
    "- Separar las funciones por pregunta mejor√≥ la claridad y mantenibilidad del c√≥digo.\n",
    "\n",
    "- Todas las funciones fueron evaluadas en tiempo y memoria, permitiendo detectar cuellos de botella y optimizar donde fue necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Mejoras futuras y evoluci√≥n\n",
    "\n",
    "#### Organizaci√≥n y Estructura del Proyecto\n",
    "\n",
    "- Reorganizar el proyecto con una estructura est√°ndar de Python:\n",
    "\n",
    "```\n",
    "tweet_data/\n",
    "‚îú‚îÄ‚îÄ data/                         # Carpeta para almacenar archivos de datos (JSON, Parquet, etc.)\n",
    "‚îú‚îÄ‚îÄ notebooks/                    # Notebooks Jupyter (.ipynb) con el desarrollo y an√°lisis\n",
    "‚îú‚îÄ‚îÄ src/                          # C√≥digo fuente del proyecto\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py               # Archivo para declarar el paquete Python\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ download_data.py          # Script para descargar el dataset desde Google Drive\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ convert_to_parquet.py     # Script para convertir el archivo JSON a formato Parquet\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ q1.py                     # L√≥gica para responder a la Pregunta 1 (top fechas y usuarios)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ q2.py                     # L√≥gica para responder a la Pregunta 2 (top emojis)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ q3.py                     # L√≥gica para responder a la Pregunta 3 (usuarios m√°s mencionados)\n",
    "‚îú‚îÄ‚îÄ tests/                        # Carpeta para pruebas unitarias utilizando pytest\n",
    "‚îú‚îÄ‚îÄ requirements.txt              # Archivo con las dependencias necesarias del proyecto\n",
    "‚îú‚îÄ‚îÄ README.md                     # Documentaci√≥n principal del proyecto\n",
    "‚îî‚îÄ‚îÄ setup.py                      # Archivo para convertir el proyecto en un paquete instalable\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### Calidad del C√≥digo\n",
    "\n",
    "- Incluir docstrings descriptivas en cada funci√≥n (entradas, salidas, descripci√≥n)\n",
    "- Escribir tests unitarios con pytest para cada funci√≥n clave (download, convert, q1, q2, q3)\n",
    "- Utilizar mocks para evitar depender de archivos reales en los tests\n",
    "\n",
    "#### Observabilidad\n",
    "\n",
    "- Automatizar an√°lisis de performance con cProfile o line_profiler\n",
    "- Generar reportes simples con m√©tricas de uso de CPU y memoria\n",
    "\n",
    "#### Empaquetado y Reproducibilidad\n",
    "\n",
    "- Crear un Makefile o script con los comandos\n",
    "- Centralizar la ejecuci√≥n en un main.py para ejecutar toda la l√≥gica paso a paso\n",
    "- Automatizar el flujo con herramientas como Prefect, Airflow o Dagster\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv tweet data)",
   "language": "python",
   "name": "venv-tweet-data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
